{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with videos under fast.ai and pytorch\n",
    "tl;dr: When working with visual data (images, video) a key element is data augmentation. Afaik right now, fastai provides superior support than pytorch and even keras/tf. I want to apply these augmentations to videos too. Also, this uses fastai 1.0.19. Things move fast so there might be easyer ways to do it.\n",
    "\n",
    "Generics\n",
    "First constraint is that 3D convolutions require data in  (𝑁,𝐶,𝐷,𝐻,𝑊)  format where N is the batch size (ignoring it for now) C is channel color, D is the time dimension (number of frames) and H,W are spatial dimensions.\n",
    "\n",
    "2nd constraint is that all the image transformations routines (vision.transform) accept  (𝐶,𝐻,𝑊)  images, without a boundary on how large C is. So convert your video into  (𝐶,𝐻,𝑊)  where  𝐶=3×𝐷 .\n",
    "\n",
    "One approach is to write your own derived vision.ImageDataset class that takes care of almost everything. See Video classification forum post. But in 1.0.19 one can \"plug in\" his/her own image_opener inside the standard image dataset.\n",
    "\n",
    "Here is another approach:\n",
    "Use Opencv + ImageClassificationBase.image_opener to load a video in vision.Image() as a 3D tensor  (𝐶,𝑊,𝐻) \n",
    "Generate regular data augmentation operations\n",
    "Construct a DatasetTfm and use it in your models\n",
    "Convert from 3D to 4D tensor, just before entering the model\n",
    "Notes\n",
    "IMPORTANT NOTE. I struggled for several hours to convert step 3 into a proprer Transform() operation, that can be added to the tfms list. Maybe smb can do it.\n",
    "\n",
    "In general, Transform operations could handle arbitrary size tensors, where the only assumption is that the last 2 plaes are (W, H). I did not dig deeper now, but in an earlier version there was a line where the tensor shape was unpacked to three values.\n",
    "\n",
    "Videos of arbitrary size will have different  𝐷  dimensions and they will not be collated into a batch. Here, I just read first 32 frames of each video. One can either code a time augmentation (eg random crop of fixed size) when the video is opened or, better, create a Transform that will do the time-aware cropping.\n",
    "\n",
    "Using opencv removes the need of offline splitting the videos in frames. (eg using ffmpeg)\n",
    "\n",
    "Hope it helps!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Some setup\n",
    "!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
    "!pip install fastai\n",
    "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import fastai\n",
    "from fastai import vision\n",
    "import cv2\n",
    "import numpy as np\n",
    "print(\"fastai ver: {}. should be >= 1.0.19\".format(fastai.__version__))\n",
    "print(cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Let's download a video here, from UCF 101 dataset. \n",
    "!wget http://crcv.ucf.edu/THUMOS14/UCF101/UCF101/v_ApplyEyeMakeup_g01_c01.avi\n",
    "vid_name = \"v_ApplyEyeMakeup_g01_c01.avi\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Implement our own video loading function that splills out a 3D vision.Image\n",
    "def load_video_using_opencv(video_file:str, how_many_frames=32) -> [vision.Image]:\n",
    "    capture = cv2.VideoCapture(video_file)\n",
    "    frames = []\n",
    "    # 1) Get a list of frames\n",
    "    for k in range(how_many_frames):\n",
    "        _, image = capture.read()\n",
    "        frames.append(image)\n",
    "    # 2) Convert them to a solid 3D tensor, of shape 3D, W, H, where D is the number of frames\n",
    "    frames = [np.transpose(f, (2, 0, 1)) for f in frames] # Jsut reorder each image to (C W H)\n",
    "    frames_np = np.vstack(frames)\n",
    "    fastai_image_3d = fastai.vision.Image(fastai.torch_core.tensor(frames_np).float().div_(255))\n",
    "\n",
    "    return fastai_image_3d\n",
    "\n",
    "fastai_3d_image = load_video_using_opencv(vid_name)\n",
    "assert fastai_3d_image is not None\n",
    "assert fastai_3d_image.shape[0] == 3 * 32 # 32 frames, each with 3 channels.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Let's create a ImageClassificationDataset\n",
    "\n",
    "fastai_ImgClassifDataset = vision.data.ImageClassificationDataset([vid_name],[0])\n",
    "fastai_ImgClassifDataset.image_opener = load_video_using_opencv  # New feature in 1.0.19, I can specify what function to use when loading \"images\".\n",
    "\n",
    "# Get a set of standard transforms and generate a transformed dataset\n",
    "tfms = vision.get_transforms(do_flip=False)[0]\n",
    "tfms_dataset = vision.data.DatasetTfm(fastai_ImgClassifDataset, tfms, False)\n",
    "\n",
    "# Pull out a sample and check its dimension\n",
    "sample = next(iter(tfms_dataset))\n",
    "print(sample) # X, Y\n",
    "augmented_image = sample[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert 3D image to 4D image\n",
    "def from_3d_to_4d(x:vision.Image)->vision.Image:\n",
    "    tensor_3d = x.px\n",
    "    sz = tensor_3d.size()\n",
    "    tensor_4d = tensor_3d.view(-1, 3, sz[1], sz[2])\n",
    "    tensor_4d = tensor_4d.permute(1, 0, 2, 3)\n",
    "    x.px = tensor_4d\n",
    "    return x\n",
    "  \n",
    "augmented_4d_image = from_3d_to_4d(augmented_image)\n",
    "print(augmented_4d_image.shape)\n",
    "\n",
    "# Unfortunately I wasn't able to create a simple transform out of from_3d_to_4d method. Weird interactions between Transform class and RandomTransform. \n",
    "# So one have to place the code above either as a manual step or as the first step in his/her model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Sanity check. Let's print some frames\n",
    "# First, a regular frame, from the original video:\n",
    "capture = cv2.VideoCapture(vid_name)\n",
    "_, image = capture.read()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now, let's see some frames from the augmented video\n",
    "\n",
    "augmented_images = []\n",
    "for d in range(0, augmented_4d_image.shape[1], 8):\n",
    "    regular_img = vision.Image(augmented_4d_image.px[:,d,:])\n",
    "    augmented_images.append(regular_img)\n",
    "   \n",
    "for i in augmented_images:\n",
    "    vision.show_image(i, figsize=(6,6))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
