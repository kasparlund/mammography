{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of mammo tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from fastai.core import *\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "import PIL\n",
    "\n",
    "from pathlib import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import colorcet as cc\n",
    "\n",
    "#from gray_image import *\n",
    "\n",
    "cmap_grey = cc.cm.linear_grey_0_100_c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = Path('../../data/mammography-data/mammography-dogscats-match-equalization-BINS-CHX/tiles')\n",
    "#print(list(path.iterdir()))\n",
    "dxrays = \"xrays\"\n",
    "dmasks = \"obs_masks\"\n",
    "dtrain = \"train\"\n",
    "dvalid = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ObsType(Enum):\n",
    "    EMPTY=0           ,\"empty\"                   # outside the breast\n",
    "    NORMAL=1          ,\"normal\"                  # tissue with no segments\n",
    "    BENIGN_CALC=2     ,\"benign_calcification\"    # tissue with benign calcification\n",
    "    MALIGNANT_CALC=3  ,\"malignant calcification\"  # tissue with malignant calcification\n",
    "    BENIGN_MASS=4     ,\"benign_mass\"              # tissue with a benign node\n",
    "    MALIGNANT_MASS=5  ,\"malignant mass\"           # tissue with a malignant node\n",
    "def enum2int(e): return e.value[0]\n",
    "\n",
    "def selectCases( cases, purpose, complexity, nb=1000 ): \n",
    "    dfAll = None\n",
    "    for k,q in complexity.items():\n",
    "        df  = cases[cases.purpose==purpose].query(q)\n",
    "        df[\"classes\"] = k\n",
    "        idx   = np.unique(np.random.uniform( low=0, high=df.shape[0], size=10*nb).astype(int))\n",
    "        idx   = idx[0:min(df.shape[0], nb, len(idx) )]\n",
    "        dfAll = dfAll.append(df.iloc[idx]) if dfAll is not None else df.iloc[idx]\n",
    "    return dfAll\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "     \n",
    "def getColorBoundsNorm(cmap, codes ):\n",
    "    cmap     = mpl.cm.get_cmap(cmap) if type(cmap)==str else cmap\n",
    "    cmaplist = [cmap(i) for i in range(len(codes))]\n",
    "    cmap     = LinearSegmentedColormap.from_list('cmap for codes', cmaplist, len(codes))\n",
    "    bounds   = np.arange(len(codes)+1)\n",
    "    norm     = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    return cmap, bounds, norm\n",
    "\n",
    "def plotImageMaskMosaic( ims, masks, codes, cmap=\"tab20\", figsize=(9,12) ):\n",
    "    nb    = len(ims)\n",
    "    ncols = int(np.sqrt(nb))\n",
    "    nrows = int(math.ceil(nb/ncols))\n",
    "\n",
    "    fig  = plt.figure(figsize=figsize)\n",
    "    \n",
    "    cmap, bounds, norm = getColorBoundsNorm( cmap, codes )\n",
    "\n",
    "    gs = gridspec.GridSpec(nrows, ncols, height_ratios=np.zeros(nrows)+ 2, wspace=0.0, hspace=0.0)\n",
    "    \n",
    "    for i in range(nb):\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(2, 1,subplot_spec=gs[i], wspace=0.0, hspace=0.0)\n",
    "        \n",
    "        ax = plt.subplot(inner[0])\n",
    "        ax.axis('off')\n",
    "        ax.imshow(ims[i])\n",
    "\n",
    "        ax = plt.subplot(inner[1])\n",
    "        ax.axis('off')\n",
    "        ax.imshow(masks[i],norm=norm, cmap=cmap)\n",
    "\n",
    "def showColorEncoding(codes, cmap=\"ctab20\") :\n",
    "    fig, ax = plt.subplots(figsize=(1, 12))\n",
    "    fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "    cmap, bounds, norm = getColorBoundsNorm( cmap, codes )\n",
    "    cb3 = mpl.colorbar.ColorbarBase(ax,cmap=cmap,norm=norm, boundaries=[-10] + bounds + [10],\n",
    "                                    ticks=bounds, spacing='uniform', orientation='vertical')\n",
    "    cb3.set_label('color encoding of class codes')\n",
    "    cb3.ax.yaxis.set_label_coords(1.25, .5)\n",
    "    cb3.set_ticklabels(codes)\n",
    "    cb3.set_ticks(0.5+bounds)\n",
    "    cb3.ax.yaxis.set_ticks_position('left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>simple_pathology</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>purpose</th>\n",
       "      <th>density</th>\n",
       "      <th>left_right</th>\n",
       "      <th>projection</th>\n",
       "      <th>obs id</th>\n",
       "      <th>...</th>\n",
       "      <th>malignant_calc</th>\n",
       "      <th>benign_mass</th>\n",
       "      <th>malignant_mass</th>\n",
       "      <th>rCenter</th>\n",
       "      <th>cCenter</th>\n",
       "      <th>tile_size</th>\n",
       "      <th>obsAreas</th>\n",
       "      <th>fnImage</th>\n",
       "      <th>fnMask</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Calc-Test_P_00127_RIGHT_CC</td>\n",
       "      <td>5375</td>\n",
       "      <td>2882</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>P_00127</td>\n",
       "      <td>test</td>\n",
       "      <td>2.0</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>35066</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1048576</td>\n",
       "      <td>48236</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_CC_tile_2435_809.png</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_CC_tile_2435_809.png</td>\n",
       "      <td>malignant_calc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Calc-Test_P_00127_RIGHT_CC</td>\n",
       "      <td>5375</td>\n",
       "      <td>2882</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>P_00127</td>\n",
       "      <td>test</td>\n",
       "      <td>2.0</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>48709</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1048576</td>\n",
       "      <td>48236</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_CC_tile_2670_1289.png</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_CC_tile_2670_1289.png</td>\n",
       "      <td>malignant_calc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Calc-Test_P_00127_RIGHT_CC</td>\n",
       "      <td>5375</td>\n",
       "      <td>2882</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>P_00127</td>\n",
       "      <td>test</td>\n",
       "      <td>2.0</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>48460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1048576</td>\n",
       "      <td>48236</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_CC_tile_2918_628.png</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_CC_tile_2918_628.png</td>\n",
       "      <td>malignant_calc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Calc-Test_P_00127_RIGHT_MLO</td>\n",
       "      <td>5669</td>\n",
       "      <td>2615</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>P_00127</td>\n",
       "      <td>test</td>\n",
       "      <td>2.0</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>23367</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1048576</td>\n",
       "      <td>41836</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_MLO_tile_4301_806.png</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_MLO_tile_4301_806.png</td>\n",
       "      <td>malignant_calc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Calc-Test_P_00127_RIGHT_MLO</td>\n",
       "      <td>5669</td>\n",
       "      <td>2615</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>P_00127</td>\n",
       "      <td>test</td>\n",
       "      <td>2.0</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>42245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1048576</td>\n",
       "      <td>41836</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_MLO_tile_3733_758.png</td>\n",
       "      <td>Calc-Test_P_00127_RIGHT_MLO_tile_3733_758.png</td>\n",
       "      <td>malignant_calc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         case_id  height  width simple_pathology patient_id  \\\n",
       "131   Calc-Test_P_00127_RIGHT_CC    5375   2882        MALIGNANT    P_00127   \n",
       "132   Calc-Test_P_00127_RIGHT_CC    5375   2882        MALIGNANT    P_00127   \n",
       "133   Calc-Test_P_00127_RIGHT_CC    5375   2882        MALIGNANT    P_00127   \n",
       "146  Calc-Test_P_00127_RIGHT_MLO    5669   2615        MALIGNANT    P_00127   \n",
       "147  Calc-Test_P_00127_RIGHT_MLO    5669   2615        MALIGNANT    P_00127   \n",
       "\n",
       "    purpose  density left_right projection  obs id       ...        \\\n",
       "131    test      2.0      RIGHT         CC       1       ...         \n",
       "132    test      2.0      RIGHT         CC       1       ...         \n",
       "133    test      2.0      RIGHT         CC       1       ...         \n",
       "146    test      2.0      RIGHT        MLO       1       ...         \n",
       "147    test      2.0      RIGHT        MLO       1       ...         \n",
       "\n",
       "    malignant_calc benign_mass malignant_mass  rCenter cCenter  tile_size  \\\n",
       "131          35066           0              0     0.55    0.46    1048576   \n",
       "132          48709           0              0     0.59    0.62    1048576   \n",
       "133          48460           0              0     0.64    0.40    1048576   \n",
       "146          23367           0              0     0.85    0.50    1048576   \n",
       "147          42245           0              0     0.75    0.49    1048576   \n",
       "\n",
       "    obsAreas                                        fnImage  \\\n",
       "131    48236   Calc-Test_P_00127_RIGHT_CC_tile_2435_809.png   \n",
       "132    48236  Calc-Test_P_00127_RIGHT_CC_tile_2670_1289.png   \n",
       "133    48236   Calc-Test_P_00127_RIGHT_CC_tile_2918_628.png   \n",
       "146    41836  Calc-Test_P_00127_RIGHT_MLO_tile_4301_806.png   \n",
       "147    41836  Calc-Test_P_00127_RIGHT_MLO_tile_3733_758.png   \n",
       "\n",
       "                                            fnMask         classes  \n",
       "131   Calc-Test_P_00127_RIGHT_CC_tile_2435_809.png  malignant_calc  \n",
       "132  Calc-Test_P_00127_RIGHT_CC_tile_2670_1289.png  malignant_calc  \n",
       "133   Calc-Test_P_00127_RIGHT_CC_tile_2918_628.png  malignant_calc  \n",
       "146  Calc-Test_P_00127_RIGHT_MLO_tile_4301_806.png  malignant_calc  \n",
       "147  Calc-Test_P_00127_RIGHT_MLO_tile_3733_758.png  malignant_calc  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purpose  observation    classes       \n",
      "test     calcification  benign_calc       169\n",
      "                        malignant_calc    167\n",
      "train    calcification  benign_calc       916\n",
      "                        malignant_calc    667\n",
      "Name: fnImage, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cases = pd.read_csv( path / \"tiles.csv\", sep=\";\", low_memory=False)\n",
    "#print(f\"cases.columns:{cases.columns}\")\n",
    "cases.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "#display(cases.tail(2))\n",
    "\n",
    "#print(\"Stat of dataset before selection\")\n",
    "#gp = cases.groupby( [\"purpose\", \"observation\", \"simple_pathology\"])[\"fnImage\"].count()\n",
    "#print( gp )\n",
    "\n",
    "codes = [obs.name.lower() for obs in ObsType]\n",
    "#print(f\"codes:{codes}\")\n",
    "\n",
    "complexity={\n",
    "            \"malignant_calc\":\"malignant_calc > 0.01*tile_size and benign_calc==0    and empty==0\",\n",
    "            \"benign_calc\":   \"benign_calc    > 0.02*tile_size and malignant_calc==0 and empty==0\"\n",
    "#            \"pHealthy\":\"pAir==0 and pBenign==0 and pMalignant==0\",\n",
    "#            \"malignant_calc\":\"malignant_calc > 0.1*tile_size and malignant_calc > 0.5*obsAreas and benign_calc==0 and empty==0\",\n",
    "#            \"pAir\":\"pAir>0.2 and pAir<1.0\", \n",
    "#            \"benign_calc\":\"benign_calc/tile_size>0.1 and benign_calc > 0.5*obsAreas and malignant_calc==0 and empty==0\"\n",
    "#            \"pMalignant\":\"pMalignant > 0.99*obsAreas and pBenign==0 and pAir==0\"\n",
    "}\n",
    "nb=1000\n",
    "\n",
    "test   = selectCases( cases, \"test\",  complexity=complexity, nb=nb//2 )\n",
    "train  = selectCases( cases, \"train\", complexity=complexity, nb=nb )\n",
    "dfData = test.append(train)\n",
    "print(\"Selected dataset\")\n",
    "display(dfData.head())\n",
    "gp = dfData.groupby( [\"purpose\", \"observation\", \"classes\"])[\"fnImage\"].count()\n",
    "print( gp )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of pixels pr class\n",
      "         empty  normal  benign_calc  malignant_calc  benign_mass  \\\n",
      "purpose                                                            \n",
      "test         0  888387        56617          103570            0   \n",
      "train        0  893745        60376           94454            0   \n",
      "\n",
      "         malignant_mass  \n",
      "purpose                  \n",
      "test                  0  \n",
      "train                 0  \n",
      "\n",
      "frequency of pixel codes\n",
      "         empty  normal  benign_calc  malignant_calc  benign_mass  \\\n",
      "purpose                                                            \n",
      "test    $0.000  $0.847       $0.054          $0.099       $0.000   \n",
      "train   $0.000  $0.852       $0.058          $0.090       $0.000   \n",
      "\n",
      "         malignant_mass  \n",
      "purpose                  \n",
      "test             $0.000  \n",
      "train            $0.000  \n",
      "code:empty weigth:0.000\n",
      "code:normal weigth:0.040\n",
      "code:benign_calc weigth:0.621\n",
      "code:malignant_calc weigth:0.339\n",
      "code:benign_mass weigth:0.000\n",
      "code:malignant_mass weigth:0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dfData = cases[np.isin(cases.case_id, selection )]\n",
    "\n",
    "#calculate the weights of the classes in the loss function\n",
    "gp = dfData.groupby([\"purpose\"])[codes].mean().astype(np.int)\n",
    "print(\"average number of pixels pr class\")\n",
    "print(gp)\n",
    "\n",
    "s = gp.sum(axis=1)\n",
    "gp.iloc[0] /= s.iloc[0]\n",
    "gp.iloc[1] /= s.iloc[1]\n",
    "print(\"\\nfrequency of pixel codes\")\n",
    "pd.options.display.float_format = '${:,.3f}'.format\n",
    "print(gp)\n",
    "\n",
    "code_weights = 1. / (gp+1e-6)\n",
    "code_weights[gp==0] = 0\n",
    "#print(\"code_weights\\n\", code_weights)\n",
    "\n",
    "s = code_weights.sum(axis=1)\n",
    "code_weights.iloc[0] /= s.iloc[0]\n",
    "code_weights.iloc[1] /= s.iloc[1]\n",
    "#print(code_weights)\n",
    "code_weights = code_weights.iloc[0].values\n",
    "[print(f\"code:{c} weigth:{w:.3f}\") for c,w in zip(codes,code_weights)]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:[0.185173 0.207602 0.217735 0.15863  0.230861]\n",
      "mean,sd: 0.20000000000000004  0.025518825045347495  0.15862956424791183  0.23086094826363715\n"
     ]
    }
   ],
   "source": [
    "def noise_zm( scale, w ):\n",
    "    s = 1+scale*(np.random.rand(len(w))-.5)*2\n",
    "    w *= s\n",
    "    return s/s.sum()\n",
    "def noise_p( scale, size ):\n",
    "    return scale*np.random.rand(size)\n",
    "def noise_weigths( scale, w ):\n",
    "    #add uniform noise at scale to non zero w\n",
    "    wn = w + scale*np.random.rand(len(w))*(w>0)\n",
    "    return wn/wn.sum()\n",
    "\n",
    "def torch_weigths_add_noise( scale, w ):\n",
    "    #add uniform noise at scale to non zero w\n",
    "    wn = w + scale*torch.rand(len(w))\n",
    "    wn[w.le(0)] = 0\n",
    "    return wn/wn.sum()\n",
    "\n",
    "def torch_weigths_mult_noise( scale, w ):\n",
    "    #add uniform noise at scale to non zero w\n",
    "    s  = 1.0 + scale*2.*( torch.rand(len(w))-0.5)  \n",
    "    w  = w*s\n",
    "    return w/w.sum()\n",
    "\n",
    "w = np.zeros(5)+1\n",
    "s = noise_zm(.2,w)\n",
    "\n",
    "print(f\"s:{s}\")\n",
    "print(f\"mean,sd: {s.mean()}  {s.std()}  {s.min()}  {s.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0264, 0.5292, 0.4444, 0.0000, 0.0000])\n",
      "code_weights:[0.       0.039575 0.620967 0.339457 0.       0.      ]\n",
      "           w:tensor([0.0000, 0.0264, 0.5292, 0.4444, 0.0000, 0.0000])\n",
      "          w2:tensor([0.0000, 0.6670, 0.8523, 1.3090, 0.0000, 0.0000])\n",
      "code_weights   -mean,sd: 0.16666666666666666  0.23654863361804015  0.0  0.6209674991278945\n",
      "code_weights+s -mean,sd: 0.3333333333333333  0.10818619048530675  0.25096006102181206  0.4861773056748708\n",
      "code_weights+s2-mean,sd: 0.3333333432674408  0.26917967200279236  0.026398219168186188  0.5292371511459351\n"
     ]
    }
   ],
   "source": [
    "scale = 0.5\n",
    "w  = noise_weigths( scale, code_weights )\n",
    "ix = w>0.\n",
    "\n",
    "wt = torch.from_numpy(code_weights.astype(np.float32))\n",
    "w2 = torch_weigths_mult_noise( scale, wt  )\n",
    "ixt = w2>0.\n",
    "print(w2)\n",
    "\n",
    "#print(.2 * (np.random.ranf()-.5) + 1)\n",
    "print(f\"code_weights:{code_weights}\" )\n",
    "print(f\"           w:{w2}\" )\n",
    "print(f\"          w2:{w2/(1e-6+wt)}\" )\n",
    "print(f\"code_weights   -mean,sd: {code_weights.mean()}  {code_weights.std()}  {code_weights.min()}  {code_weights.max()}\")\n",
    "print(f\"code_weights+s -mean,sd: {w[ix].mean()}  {w[ix].std()}  {w[ix].min()}  {w[ix].max()}\")\n",
    "print(f\"code_weights+s2-mean,sd: {w2[ixt].mean()}  {w2[ixt].std()}  {w2[ixt].min()}  {w2[ixt].max()}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "files = list((path/dvalid/dxrays).iterdir())\n",
    "r     = dfData.iloc[0]\n",
    "fn_im = path/(dtrain if r.purpose==\"train\" else dvalid)/dxrays/r.fnImage\n",
    "im    = open_image_16bit2rgb(fn_im)\n",
    "show_image( im, figsize=(5,5) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fn_msk = path / (dtrain if r.purpose==\"train\" else dvalid)/dmasks/r.fnImage\n",
    "mask   = open_mask(fn_msk)\n",
    "mask.show(figsize=(5,5), cmap=\"tab20\", alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSegmentationTransforms():\n",
    "    max_zoom     = 1.1\n",
    "    max_rotate   = 0.1\n",
    "    brightnes_range = (0.8, 1.0)\n",
    "    contrast_range  = (0.85, 1.05)\n",
    "    rotate_range    = (-10, 10) # degrees\n",
    "    warp_range     = (-0.1, 0.1)\n",
    "    tfmTrain,tfmValid =[],[]\n",
    "    tfmTrain.append(crop_pad())\n",
    "    #tfmTrain.append( rand_crop() )\n",
    "    tfmTrain.append( dihedral_affine() )\n",
    "    \n",
    "    tfmTrain.append( rotate(degrees=rotate_range, p=.5) )\n",
    "    \"\"\"\n",
    "    #tfmTrain.append( flip_affine(p=0.5) )\n",
    "    \n",
    "    #tfmTrain.append( skew( np.arange, p=0.75) )\n",
    "    tfmTrain.append( symmetric_warp(magnitude=warp_range, p=0.5) )\n",
    "    #tfmTrain.append( perspective_warp(magnitude=warp_range, p=0.25) )\n",
    "    \n",
    "    #tfmTrain.append( rand_zoom(scale=(1.,max_zoom), p=0.75) )\n",
    "\n",
    "    tfmTrain.append( jitter(magnitude=0.01) )\n",
    "    \"\"\"\n",
    "    \n",
    "    if brightnes_range is not None: tfmTrain.append( brightness(change=brightnes_range, p=0.25) )\n",
    "    if contrast_range is not None:  tfmTrain.append( contrast(scale=contrast_range,     p=0.25) )\n",
    "        \n",
    "    tfmValid.append(crop_pad())\n",
    "    #tfmValid.append( flip_affine(p=0.5) )\n",
    "    return (tfmTrain, tfmValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixTrain  = dfData.purpose==\"train\"\n",
    "ixValid  = ixTrain == False\n",
    "\n",
    "train  = join_paths(dfData.fnImage[ixTrain].values, path/dtrain/dxrays)\n",
    "valid  = join_paths(dfData.fnImage[ixValid].values, path/dvalid/dxrays)\n",
    "\n",
    "get_y_fn   = lambda x: x.parents[1]/dmasks/x.name \n",
    "valid_func = lambda x: x.parts[-3] == dvalid\n",
    "\n",
    "def open_image_16bit2rgb( fn ): \n",
    "    a = np.asarray(PIL.Image.open( fn ))\n",
    "    a = np.expand_dims(a,axis=2)\n",
    "    a = np.repeat(a, 3, axis=2)\n",
    "    return Image( pil2tensor(a, np.float32 ).div(65535) )\n",
    "\n",
    "vision.data.open_image = open_image_16bit2rgb\n",
    "\n",
    "src = (SegmentationItemList( list(train+valid), create_func=open_image_16bit2rgb, path=\"\")\n",
    "                           .split_by_valid_func(valid_func)\n",
    "                           .label_from_func(get_y_fn, classes=codes)           \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,size=16,224\n",
    "data = ( src.transform(getSegmentationTransforms(), size=size, tfm_y=True)\n",
    "            .databunch(bs=bs)\n",
    "            .normalize(imagenet_stats) )\n",
    "#data.show_batch(2, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fastai.core\n",
    "#from fastai.layers import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def accuracy_(input, target):\n",
    "    #The input.argmax(dim=1) selects the winner class pr pixel, thereby reducing \n",
    "    #the input shape from (bs, classes, width, height) => (bs, width, height)\n",
    "    #We, therefore, have to reshape the target tensor from (bs, 1, width, height) to (bs, width, height)\n",
    "    sz     = target.size()\n",
    "    target = target.reshape( (sz[0],sz[2],sz[3]) )\n",
    "    return (input.argmax(dim=1).flatten()==target.flatten()).float().mean()\n",
    "\n",
    "def torch_weigths_add_noise( scale, w ):\n",
    "    #add uniform noise at scale to non zero w\n",
    "    wn = w + scale*torch.rand(len(w)).cuda()\n",
    "    wn[w.le(0)] = 0\n",
    "    return wn/wn.sum()\n",
    "\n",
    "def torch_weigths_mult_noise( scale, w ):\n",
    "    #add uniform noise at scale to non zero w\n",
    "    s  = 1.0 + scale*2.*( torch.rand(len(w)).cuda()-0.5)  \n",
    "    w  = w*s\n",
    "    return w/w.sum()\n",
    "\"\"\"\n",
    "class WeighedCrossEntropy(CrossEntropyFlat):\n",
    "    def __init__( self, weights ):\n",
    "        super().__init__(weight=torch.from_numpy( np.asarray(weights,np.float32) ).cuda() )\n",
    "        self.register_buffer(\"weight_mask\",torch.from_numpy( (weights>0.0).astype(np.float32) ).cuda())\n",
    "                             \n",
    "    def forward(self, input:Tensor, target:Tensor) -> Rank0Tensor:\n",
    "        scale  = .5\n",
    "        w      = torch_weigths_mult_noise(scale, self.weight)\n",
    "        n,c,*_ = input.shape\n",
    "        return F.cross_entropy(input.view(n, c, -1), target.view(n, -1), weight=w,\n",
    "                               ignore_index=self.ignore_index, reduction=self.reduction)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    dicemsg=\"\"\n",
    "    @staticmethod\n",
    "    def dice_loss_1ch(input, target):\n",
    "        smooth = 1.\n",
    "        input = torch.sigmoid(input)\n",
    "        iflat = input.view(-1)\n",
    "        tflat = target.view(-1)\n",
    "        #DiceLoss.dicemsg = (f\"shape of input:{input.shape}, target:{target.shape} \\\n",
    "        #                     input.type():{input.type()} iflat.type():{iflat.type()}  tflat.type():{tflat.type()}\")\n",
    "        intersection = (iflat * tflat).sum()\n",
    "        return 1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "        \n",
    "        #prediction = self.sigmoid(output)\n",
    "        #return 1 - 2 * torch.sum(prediction * target) / (torch.sum(prediction) + torch.sum(target) + 1e-7)\n",
    "        \n",
    "    @staticmethod\n",
    "    def dice_loss_nch(input, target):\n",
    "        dlc = 0\n",
    "        bs, n_classes = input.shape[:2] \n",
    "        target = target.squeeze().float()\n",
    "        n_real_classes=0\n",
    "        for i in range(n_classes):\n",
    "            if code_weights[i] > 0:\n",
    "                dlc += DiceLoss.dice_loss_1ch(input[:,i], target)\n",
    "                n_real_classes+=1\n",
    "        return dlc/n_real_classes/bs\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #super(DiceLoss, self).__init__()\n",
    "        self.smooth = 1.\n",
    "                             \n",
    "    def forward(self, input, target):\n",
    "        #input = torch.sigmoid(input)\n",
    "        #preds = input.argmax(dim=1)\n",
    "        #preds = preds.view(preds.shape[0], -1, preds.shape[1], preds.shape[2] )\n",
    "        #preds = preds.view_as(target)\n",
    "        #DiceLoss.dicemsg = (f\"shape of input:{input.shape} of target:{target.shape} preds:{preds.shape} preds\\n{preds}\")\n",
    "        return DiceLoss.dice_loss_nch(input,target)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(np.float32)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code:empty weigth:0.000\n",
      "code:normal weigth:0.040\n",
      "code:benign_calc weigth:0.621\n",
      "code:malignant_calc weigth:0.339\n",
      "code:benign_mass weigth:0.000\n",
      "code:malignant_mass weigth:0.000\n"
     ]
    }
   ],
   "source": [
    "[print(f\"code:{c} weigth:{w:.3f}\") for c,w in zip(codes,code_weights)]\n",
    "learn = Learner.create_unet(data, models.resnet34, loss_func= DiceLoss(), # FocalLoss(gamma=2.,alpha=.25), # WeighedCrossEntropy(code_weights),  \n",
    "                            metrics=accuracy_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DiceLoss.dicemsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "lr_find(learn)\n",
    "n=gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='6', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      16.67% [1/6 00:20<01:44]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy_</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.009404</th>\n",
       "    <th>-0.004333</th>\n",
       "    <th>0.098531</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='40' class='' max='99', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      40.40% [40/99 00:08<00:12 0.0014]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr=2e-3\n",
    "learn.fit_one_cycle(6, slice(lr))\n",
    "n=gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('resnet34-stage-1')\n",
    "n=gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('resnet34-stage-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.show_results(rows=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "bs,size=4,448\n",
    "data = (src.transform(getSegmentationTransforms(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "learn = Learner.create_unet(data, models.resnet34, loss_func= WeighedCrossEntropy(code_weights), \n",
    "                            metrics=accuracy_)\n",
    "learn.loss_func= WeighedCrossEntropy(code_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-3\n",
    "learn.fit_one_cycle(6, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(), plt.show(), learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('resnet34-stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(rows=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GO BIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs,size=4,448\n",
    "bs,size=4//2,2*448\n",
    "data = (src.transform(getSegmentationTransforms(), size=size, tfm_y=True)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "learn = Learner.create_unet(data, models.resnet34, loss_func= WeighedCrossEntropy(code_weights), \n",
    "                            metrics=accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('resnet34-stage-2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lrs = slice(1e-7,lr/5)\n",
    "learn.fit_one_cycle(6, lrs, moms=(0.97,0.92) )\n",
    "n=gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(), plt.show(), learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(rows=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('resnet34-stage-big-good');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(learn, n:int, ds_type=DatasetType.Valid):\n",
    "    ds = learn.dl(ds_type).dataset\n",
    "    xys, y_p, preds,  = [],[],None\n",
    "    ip = 0\n",
    "    for i in range(n):\n",
    "        xys.append( ds[i] )\n",
    "        if preds is None or preds.shape[0] <= ip:\n",
    "            preds = learn.pred_batch(ds_type)\n",
    "            ip    = 0\n",
    "        y_p.append(preds[ip])\n",
    "        ip += 1\n",
    "        \n",
    "    x,y,p=[],[],[]\n",
    "    for i, xy in enumerate(xys):\n",
    "        x_,y_ = xy\n",
    "        x.append(x_)\n",
    "        y.append(y_)\n",
    "        p.append(y_.reconstruct_output(y_p[i],x_) )\n",
    "    return x,y,p\n",
    "    #print(f\"{len(x)}, {len(y)}, {len(p)}\")\n",
    "    \n",
    "xs,ys,y_ps = getResults(learn,n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[0].show(), ys[0].show(),  y_ps[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=9\n",
    "ims,masks = [],[]\n",
    "for i in range(n):\n",
    "    x,y = data.train_ds[i]\n",
    "    ims.append(   image2np(x.data) ) \n",
    "    masks.append( image2np(y.data) )\n",
    "print(\"classes i mask:\"), [print(f\"mask {i}:{np.unique(m)}\") for i,m in enumerate(masks)]\n",
    "\n",
    "cmap=plt.cm.tab10\n",
    "showColorEncoding(codes, cmap=cmap)     \n",
    "plotImageMaskMosaic(ims, masks, codes, cmap=cmap, figsize=(10,14))\n",
    "\n",
    "print(data.classes) len(data.classes),data.c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lr_find(learn, start_lr = 1e-04,end_lr=100.0 )\n",
    "learn.recorder.plot(skip_end=4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "bs   = [16,4,2] \n",
    "size = np.asarray([1, 2, 3])*224\n",
    "lr   = [0.001, 0.005, 0.01]\n",
    "msg = []\n",
    "learn = None\n",
    "for i in range(0,len(lr)):\n",
    "    data = ImageDataBunch.create(dsTrain, dsValid, ds_tfms=tfms, tfm_y=True, bs=bs[i], size=size[i] )\n",
    "    data.normalize(imagenet_stats)\n",
    "\n",
    "    if learn is None : \n",
    "        learn = Learner.create_unet(data, models.resnet50, loss_func= WeighedCrossEntropy(code_weights), \n",
    "                                    metrics=accuracy_)\n",
    "    else: \n",
    "        learn.data = data\n",
    "    \n",
    "    train_ds_im_size,valid_ds_im_size = learn.data.train_ds.kwargs[\"size\"], learn.data.valid_ds.kwargs[\"size\"]\n",
    "    msg.append( f\"batch size:{bs[i]} image size for train_ds { train_ds_im_size} - valid_ds for {valid_ds_im_size}\" )\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    learn.fit_one_cycle(len(lr)-i, max_lr=lr[i])\n",
    "\n",
    "msg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plotTensor( im ):       \n",
    "    colorMap = plt.cm.gray if im.ndim == 2 else None \n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8)) \n",
    "    plt.imshow(im, cmap=colorMap)    \n",
    "    plt.show()\n",
    "    \n",
    "def createFilterImage( weights ) :   \n",
    "    #scale to 0-1 and put the channel in the last as expect by matplotlib\n",
    "    weights = 0.5*np.copy(weights+1.0)\n",
    "    weights = np.rollaxis(weights, 1, 4)\n",
    "    \n",
    "    filter_dim = weights.shape[1]\n",
    "    dim = int( 0.5+ np.sqrt( weights.shape[0] ) ) * filter_dim\n",
    "    \n",
    "    im = np.zeros([dim,dim,weights.shape[3]])\n",
    "    for i in range(8):\n",
    "        io = i*filter_dim\n",
    "        for j in range(8):\n",
    "            jo = j*filter_dim            \n",
    "            i_filter = i*(filter_dim+1) + j\n",
    "            #print( \"\\ni_filter: {}:  i_off: {} - {},  j_off: {} - {}\",  i_filter, io, (io+filter_dim), jo,(jo+filter_dim) )\n",
    "            im[io:(io+filter_dim), jo:(jo+filter_dim), :] = weights[i_filter,:,:,:]\n",
    "\n",
    "    im = np.squeeze(im)        \n",
    "    return im, filter_dim\n",
    "\n",
    "def getLayers( model ): \n",
    "    layers=[]\n",
    "    for g in list( model.children() ):\n",
    "        layers.extend( list(g.children()) )\n",
    "    return layers\n",
    "\n",
    "def WeightsAsImage(model) :\n",
    "    layers = getLayers(model)\n",
    "    l      = layers[0]\n",
    "    \n",
    "    w = l.weight.data.cpu().numpy()\n",
    "    im, filter_dim = createFilterImage(w)\n",
    "    return im\n",
    "    \n",
    "def printWeight( model, i0=0, i1=1 ):\n",
    "    layers = getLayers(model)\n",
    " \n",
    "    for l in layers[i0:i1]:        \n",
    "        print(\"meatadata for layer: \", l)\n",
    "        w = l.weight.data.cpu().numpy()\n",
    "        im, filter_dim = createFilterImage(w)\n",
    "        print(\"One image with all n filterweight with size \", w.shape[0], filter_dim, filter_dim )\n",
    "        #print(\"\\nweights.shape :\", w.shape, \" - weights:\\n\", w)\n",
    "        plotTensor(im)\n",
    "    \n",
    "printWeight(learn.model)\n",
    "\n",
    "weightsBeforeUnfreeze =  WeightsAsImage(learn.model)\n",
    "weightsBeforeUnfreeze.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gc.collect()\n",
    "\n",
    "bs   = [8,2] \n",
    "size = np.asarray([1, 2])*224\n",
    "lr   = [0.0001, 0.001]\n",
    "msg = []\n",
    "learn = None\n",
    "for i in range(0,2):\n",
    "    data = ImageDataBunch.create(dsTrain, dsValid, ds_tfms=tfms, tfm_y=True, bs=bs[i], size=size[i] )\n",
    "    data.normalize(imagenet_stats)\n",
    "\n",
    "    if learn is None : \n",
    "        learn = Learner.create_unet(data, models.resnet50, loss_func= WeighedCrossEntropy(code_weights), \n",
    "                                    metrics=accuracy_)\n",
    "    else: \n",
    "        learn.data = data\n",
    "        \n",
    "    if i==0 : learn.unfreeze()\n",
    "\n",
    "    train_ds_im_size,valid_ds_im_size = learn.data.train_ds.kwargs[\"size\"], learn.data.valid_ds.kwargs[\"size\"]\n",
    "    msg.append( f\"batch size:{bs[i]} image size for train_ds { train_ds_im_size} - valid_ds for {valid_ds_im_size}\" )\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    lrs = slice(1e-6, lr[i])\n",
    "    learn.fit_one_cycle(len(lr)-i, lrs, wd=1e-3)    \n",
    "\n",
    "msg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#learn.unfreeze()\n",
    "\n",
    "#unfreeze first layer\n",
    "layers = getLayers(learn.model)\n",
    "for param in layers[0].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#learn.fit_one_cycle(6, slice(lr_last_layer*1e-4, lr_last_layer * 1e-1) )\n",
    "learn.fit_one_cycle(12, slice(lr_last_layer*1e-2, lr_last_layer * 0) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weightsAfterTrainingInputLayer = WeightsAsImage(learn.model)\n",
    "fig,axes = plt.subplots(nrows=1, ncols=2, figsize =(12,12), dpi=100)\n",
    "axes[0].imshow(weightsBeforeUnfreeze)  \n",
    "axes[1].imshow(weightsAfterTrainingInputLayer)  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "diff = weightsAfterTrainingInputLayer-weightsBeforeUnfreeze\n",
    "diff = (diff-diff.min())/(diff.max()-diff.min())\n",
    "plotTensor(diff)\n",
    "weightsAfterTrainingInputLayer-weightsBeforeUnfreeze"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max_lr = lr_last_layer*0.75\n",
    "min_lr = max_lr*1e-4\n",
    "print(f\"unfreeze learning slice: {min_lr:.2e} to {max_lr:.2e} \")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10,  max_lr=slice(min_lr,max_lr))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.recorder.plot_losses(), plt.show()\n",
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fnImage = path/valid/xrays/\n",
    "\n",
    "fnImages = join_paths(dfData.fnImage.values, path/d_im)\n",
    "fnMasks  = join_paths(dfData.fnMask.values, path/d_msk)\n",
    "ixTrain  = dfData.purpose==\"train\"\n",
    "ixValid  = ixTrain == False\n",
    "dsTrain  = SegmentationDataset(join_paths(dfData.fnImage[ixTrain].values, path/d_im), \n",
    "                               join_paths(dfData.fnImage[ixTrain].values, path/d_msk), \n",
    "                               codes)\n",
    "dsValid  = SegmentationDataset(join_paths(dfData.fnImage[ixValid].values, path/d_im), \n",
    "                               join_paths(dfData.fnImage[ixValid].values, path/d_msk), \n",
    "                               codes)\n",
    "dsTrain.image_opener = dsValid.image_opener = open_image_16bit2rgb\n",
    "dsTrain.mask_opener  = dsValid.mask_opener  = open_mask_special"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
